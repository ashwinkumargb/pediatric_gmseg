{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data creation for Pediatric GM Segmentation\n",
    "1. Understand data skew\n",
    "2. Obtain directories to put in training, validation, and test sets\n",
    "3. Create numpy blocks of training, validation, and testing sets\n",
    "4. Validate whether datasets match original numpy arrays from directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Skew (1) and getting directory values (2)\n",
    "\n",
    "Split data into 60/20/20 breakdown for train, val, and test respectively. Handled Skew in DTI_Normal_level.xlsx and balanced Lower Thoracic, Thoracic, and Cervical data. Though cervical data predominates, training data is fairly balanced. Val and testing are similarly balanced. *Train_dirs, val_dirs, and test_dirs* include the directories for dataset that handles skew. There will be imperfections, but this is a relatively balanced method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dirs = ['140', '162', '162_second', '172', '172_second', '166_second', \\\n",
    "    '195', '21', '106', '208', '205', '237', '88', '93', '61', '113', '92_second', \\\n",
    "    '114', '143', '163', '153', '90', '125', '105', '175', '144', '58', '196', \\\n",
    "    '102', '235', '197', '207', '191', '124', '189', '226', '157', '194', \\\n",
    "    '180', '210', '187']\n",
    "\n",
    "val_dirs = ['179_second', '199', '82', '142', '115', '135', '247', '127', \\\n",
    "    '161', '55', '181', '244', '150']\n",
    "\n",
    "test_dirs = ['173_second', '86', '53', '70', '56', '160', '116', '52', \\\n",
    "    '155', '174', '169', '148', '201', '112']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training, validation, and testing datasets (3)\n",
    "\n",
    "Going to go through directories and first append the numpy arrays to a list and then convert them a numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "working_dir = '/Users/captain/Documents/combined_roi/pediatric_gmseg'\n",
    "mFFE_crop = 'mFFE_crop_r.nii.gz'\n",
    "mFFE_gmseg = 'mFFE_crop_r_gmseg.nii.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "train_data = []\n",
    "train_targets = []\n",
    "\n",
    "for dir in train_dirs:\n",
    "    os.chdir(os.path.join(working_dir, dir))\n",
    "    train_data.append(nib.load(mFFE_crop).get_fdata())\n",
    "    train_targets.append(nib.load(mFFE_gmseg).get_fdata())\n",
    "\n",
    "#Return to base directory\n",
    "os.chdir(working_dir)\n",
    "\n",
    "#Turn lists into numpy arrays\n",
    "train_data = np.concatenate(train_data, axis=2)\n",
    "train_targets = np.concatenate(train_targets, axis=2)\n",
    "\n",
    "#Save numpy arrays\n",
    "np.save('train_data.npy', train_data)\n",
    "np.save('train_targets.npy', train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation data\n",
    "val_data = []\n",
    "val_targets = []\n",
    "\n",
    "for dir in val_dirs:\n",
    "    os.chdir(os.path.join(working_dir, dir))\n",
    "    val_data.append(nib.load(mFFE_crop).get_fdata())\n",
    "    val_targets.append(nib.load(mFFE_gmseg).get_fdata())\n",
    "\n",
    "#Return to base directory\n",
    "os.chdir(working_dir)\n",
    "\n",
    "#Turn lists into numpy arrays\n",
    "val_data = np.concatenate(val_data, axis=2)\n",
    "val_targets = np.concatenate(val_targets, axis=2)\n",
    "\n",
    "#Save numpy arrays\n",
    "np.save('val_data.npy', val_data)\n",
    "np.save('val_targets.npy', val_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing data\n",
    "test_data = []\n",
    "test_targets = []\n",
    "\n",
    "for dir in test_dirs:\n",
    "    os.chdir(os.path.join(working_dir, dir))\n",
    "    test_data.append(nib.load(mFFE_crop).get_fdata())\n",
    "    test_targets.append(nib.load(mFFE_gmseg).get_fdata())\n",
    "\n",
    "#Return to base directory\n",
    "os.chdir(working_dir)\n",
    "\n",
    "#Turn lists into numpy arrays\n",
    "test_data = np.concatenate(test_data, axis=2)\n",
    "test_targets = np.concatenate(test_targets, axis=2)\n",
    "\n",
    "#Save numpy arrays\n",
    "np.save('test_data.npy', test_data)\n",
    "np.save('test_targets.npy', test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating .npy arrays (4)\n",
    "\n",
    "Validating if .npy truly contains the intended data by comparing it to the NifTi in the original directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return to base dir\n",
    "os.chdir(working_dir)\n",
    "\n",
    "#Performing Validation to see whether data is the same\n",
    "samp_data = np.load('test_data.npy')\n",
    "samp_targets = np.load('test_targets.npy')\n",
    "start_slice = 0\n",
    "end_slice = 14\n",
    "data_equal = []\n",
    "target_equal = []\n",
    "for dir in test_dirs:\n",
    "    os.chdir(os.path.join(working_dir, dir))\n",
    "    data_equal.append(np.array_equal(samp_data[:,:, start_slice:end_slice],nib.load(mFFE_crop).get_fdata()))\n",
    "    target_equal.append(np.array_equal(samp_targets[:,:, start_slice:end_slice],nib.load(mFFE_gmseg).get_fdata()))\n",
    "    start_slice += 14\n",
    "    end_slice += 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating an SCT GMSeg data for model comparison\n",
    "\n",
    "Go through the testing directories and calculating GMSeg. Get arrays and create a numpy array for GMSeg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Array to append SCT gmseg to\n",
    "sct_gm = []\n",
    "\n",
    "# file name\n",
    "sct_gm_seg = 'mFFE_crop_r_gmseg_test.nii.gz'\n",
    "\n",
    "#Go through the testing directories and compute gm seg\n",
    "for dir in test_dirs:\n",
    "    print(dir)\n",
    "    os.chdir(os.path.join(working_dir, dir))\n",
    "    subprocess.run(['sct_deepseg_gm', '-i', 'mFFE_crop_r.nii.gz', '-o', sct_gm_seg])\n",
    "    sct_gm.append(nib.load(sct_gm_seg).get_fdata())\n",
    "\n",
    "#Return to base directory\n",
    "os.chdir(working_dir)\n",
    "\n",
    "#Turn lists into numpy arrays\n",
    "sct_targets = np.concatenate(sct_gm, axis=2)\n",
    "\n",
    "#Save numpy arrays\n",
    "np.save('sct_targets.npy', sct_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate SCT gmseg target files\n",
    "\n",
    "Load numpy files and perform comparison similar to train, val, and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#return to base dir\n",
    "os.chdir(working_dir)\n",
    "\n",
    "# file name\n",
    "sct_gm_seg = 'mFFE_crop_r_gmseg_test.nii.gz'\n",
    "\n",
    "#Performing Validation to see whether data is the same\n",
    "sct_targets = np.load('sct_targets.npy')\n",
    "start_slice = 0\n",
    "end_slice = 14\n",
    "target_equal = []\n",
    "for dir in test_dirs:\n",
    "    os.chdir(os.path.join(working_dir, dir))\n",
    "    target_equal.append(np.array_equal(sct_targets[:,:, start_slice:end_slice], nib.load(sct_gm_seg).get_fdata()))\n",
    "    start_slice += 14\n",
    "    end_slice += 14\n",
    "\n",
    "print(target_equal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c11f87f5fba9d53f7f4480496793396c64d84d5f4a215684543ed784e59a5b5f"
  },
  "kernelspec": {
   "display_name": "Python 3.6.10 ('py36')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
